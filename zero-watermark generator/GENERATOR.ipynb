{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset,DataLoader\n",
    "\n",
    "import torchvision.utils as utils\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import cv2\n",
    "\n",
    "import copy\n",
    "import glob\n",
    "import torch.utils.data as udata\n",
    "import h5py\n",
    "import cv2\n",
    "import random\n",
    "import pywt\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import math\n",
    "from skimage.measure.simple_metrics import compare_psnr\n",
    "import scipy.io as sio\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "plt.rcParams['font.sans-serif'] = [u'SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#cpu/GPU训练\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    device = \"cuda:0\" \n",
    "    dtype = torch.cuda.FloatTensor \n",
    "    imsize = 128 \n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    dtype = torch.FloatTensor\n",
    "    # desired size of the output image\n",
    "    imsize = 128  # use small size if no gpu\n",
    "print(use_cuda)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired size of the output image 数据预处理\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize([imsize, imsize]),  # scale imported image\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name) \n",
    "    image = Variable(loader(image)) #1,128,128 +grad+ grad_fn\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = image.unsqueeze(0) #\n",
    "    return image\n",
    "\n",
    "unloader = transforms.ToPILImage()  # reconvert into PIL image \n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.clone().to(device) # we clone the tensor to not do changes on it\n",
    "    image = image.view(3, imsize, imsize)  # remove the fake batch dimension\n",
    "    image = unloader(image) #(W,H)\n",
    "    plt.imshow(image) #显示图片\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated 动态更新图像，感觉没啥意义？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE 内容损失，采用VGG网络conv4_1层输出的均方差损失 ，内容图为西电图标 自定义损失\n",
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target, weight):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        self.target = target.detach() * weight \n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.weight = weight\n",
    "        self.criterion = nn.MSELoss() #损失函数\n",
    "\n",
    "    def forward(self, input): #前向传播 计算梯度\n",
    "        self.loss = self.criterion(input * self.weight, self.target) #输入和目标都*权重，然后计算均方差损失\n",
    "        self.output = input #返回不变？\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, retain_graph=True):  #反向传播 最小化损失\n",
    "        self.loss.backward(retain_graph=retain_graph) #保存中间变量，不然会直接在优化完这个损失之后清空变量\n",
    "        return self.loss #返回损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GM 矩阵的计算  用于计算风格损失\n",
    "class GramMatrix(nn.Module):\n",
    "\n",
    "    def forward(self, input):\n",
    "        a, b, c, d = input.size()  # a=batch size(=1)\n",
    "        # b=number of feature maps chnnel number (h*w)\n",
    "        # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "        features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL (chnnel,h*w)\n",
    "\n",
    "        G = torch.mm(features, features.t())  # compute the gram product 对特征向量以及他的转置矩阵做内积(点乘)，就是grim matrix\n",
    "\n",
    "        # we 'normalize' the values of the gram matrix\n",
    "        # by dividing by the number of element in each feature maps.\n",
    "        return G.div(a * b * c * d) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleLoss(nn.Module): #f风格损失\n",
    "\n",
    "    def __init__(self, target, weight):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = target.detach() * weight\n",
    "        self.weight = weight\n",
    "        self.gram = GramMatrix()\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def forward(self, input): #前向传播参数\n",
    "        self.output = input.clone()\n",
    "        self.G = self.gram(input)\n",
    "        self.G.mul_(self.weight)\n",
    "        self.loss = self.criterion(self.G, self.target)\n",
    "        return self.output\n",
    "\n",
    "    def backward(self, retain_graph=True): #反向传播损失\n",
    "        self.loss.backward(retain_graph=retain_graph)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = models.vgg19(pretrained=True).features\n",
    "# move it to the GPU if possible:\n",
    "if use_cuda:\n",
    "    cnn = cnn.to(device)#使用VGG网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = ['conv_4'] #内容特征层\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4']\n",
    "#style_layers_default = [ 'conv_2', 'conv_4', 'conv_5','conv_6',]\n",
    "#style_layers_default = ['conv_1', 'conv_3', 'conv_5', 'conv_9', 'conv_13'] #风格特征层\n",
    "def get_style_model_and_losses(cnn, style_img, content_img,\n",
    "                               style_weight=1000, content_weight=1,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    model = nn.Sequential()  # the new Sequential module network 初始化构建一个网络\n",
    "    gram = GramMatrix()  # we need a gram module in order to compute style targets\n",
    "    print('gram', gram)\n",
    "    # move these modules to the GPU if possible:\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        gram = gram.cuda()\n",
    "\n",
    "    i = 1\n",
    "    for layer in list(cnn):\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            name = \"conv_\" + str(i)\n",
    "            model.add_module(name, layer) #我们之前只初始化了损失函数，现在，我们在网络模型中加入一个个网路层，现在遍历VGG，把他的层都加入\n",
    "            #我的模型里面\n",
    "            #print('新加入一层，名字为{}'.format(name))\n",
    "\n",
    "            if name in content_layers:\n",
    "                # add content loss:\n",
    "                target = model(content_img).clone()\n",
    "                content_loss = ContentLoss(target, content_weight)\n",
    "                model.add_module(\"content_loss_\" + str(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "                #print('我把这一层作为内容层{}'.format(name))\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).clone()\n",
    "                # style_img经过提取后的特征展开为向量，然后构建gram矩阵\n",
    "                target_feature_gram = gram(target_feature)\n",
    "                style_loss = StyleLoss(target_feature_gram, style_weight)\n",
    "                model.add_module(\"style_loss_\" + str(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "                #print('我把这一层作为风格层{}'.format(name))\n",
    "\n",
    "        if isinstance(layer, nn.ReLU):\n",
    "            name = \"relu_\" + str(i)\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        if isinstance(layer, nn.MaxPool2d):\n",
    "            name = \"pool_\" + str(i)\n",
    "            model.add_module(name, layer)  # ***\n",
    "\n",
    "    return model, style_losses, content_losses "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_input_param_optimizer(input_img): #优化器\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    input_param = nn.Parameter(input_img.data)\n",
    "    optimizer = optim.LBFGS([input_param])  #目前最好的优化方法 低存储BFGS\n",
    "    return input_param, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, content_img, style_img, input_img, num_steps=300,\n",
    "                       style_weight=500, content_weight=1):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        style_img, content_img, style_weight, content_weight)\n",
    "    input_param, optimizer = get_input_param_optimizer(input_img)\n",
    "    #原来对每一张图像进行风格迁移，代表需要对每张图像都建立一个模型，都计算损失，都优化模型，都训练n次\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_param.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_param)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.backward()\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            \n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.data.to('cpu').numpy(), content_score.data.to('cpu').numpy()))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    input_param.data.clamp_(0, 1)\n",
    "\n",
    "    return model, style_losses, content_losses, input_param.data #返回优化后的网络模型损失和风格迁移结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cnn4():\n",
    "    cnn = models.vgg19(pretrained=True).features\n",
    "    # move it to the GPU if possible:\n",
    "    if use_cuda:\n",
    "        cnn = cnn.cuda()\n",
    "\n",
    "    model_vgg=cnn[0:8]\n",
    "    if use_cuda:\n",
    "        model_vgg = model_vgg.cuda()\n",
    "    return model_vgg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_unloader(tensor):\n",
    "    image = tensor.clone().to(device)  # we clone the tensor to not do changes on it\n",
    "    image = image.view(3, imsize, imsize)  # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# style_img = image_loader(LOAD_PATH).type(dtype) #.squeeze()\n",
    "# #load content\n",
    "# content_img = image_loader(CONTENT_PATH).type(dtype) #.squeeze()\n",
    "# input_img = content_img.clone()\n",
    "\n",
    "# model, style_losses, content_losses, output = run_style_transfer(cnn, content_img, style_img, input_img, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = \"./style\"\n",
    "SAVE_PATH = \"./zero-watermark\"\n",
    "CONTENT_PATH = \"content/l2.png\"\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.mkdir(SAVE_PATH)  #创建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_lst = os.listdir(LOAD_PATH)\n",
    "img_lst.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 0.849238 Content Loss: 7.607591\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 0.828870 Content Loss: 7.226240\n",
      "\n",
      "[2/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 2.299284 Content Loss: 5.733112\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 1.196197 Content Loss: 5.552648\n",
      "\n",
      "[3/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 0.864887 Content Loss: 7.657245\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 0.813435 Content Loss: 7.258070\n",
      "\n",
      "[4/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1.803954 Content Loss: 5.322325\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 0.717991 Content Loss: 5.112463\n",
      "\n",
      "[5/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 2.168500 Content Loss: 5.868726\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 1.218965 Content Loss: 5.707106\n",
      "\n",
      "[6/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1.007298 Content Loss: 4.968815\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 0.715517 Content Loss: 4.810609\n",
      "\n",
      "[7/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1.592724 Content Loss: 5.109880\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 0.632126 Content Loss: 4.983813\n",
      "\n",
      "[8/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1.097972 Content Loss: 5.025748\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 0.712493 Content Loss: 4.906319\n",
      "\n",
      "[9/9]\n",
      "Building the style transfer model..\n",
      "gram GramMatrix()\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 1.592724 Content Loss: 5.109880\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 0.632126 Content Loss: 4.983813\n",
      "\n"
     ]
    }
   ],
   "source": [
    "STEP = 100\n",
    "i = 1\n",
    "for img_name in img_lst:\n",
    "    print(\"[%d/%d]\" % (i, len(img_lst)))\n",
    "    i = i + 1\n",
    "    if os.path.splitext(img_name)[-1] != '.png':\n",
    "        continue\n",
    "    img_path = os.path.join(LOAD_PATH, img_name)\n",
    "    style_img = image_loader(img_path).type(dtype).to(device)#.squeeze()\n",
    "    #load content\n",
    "    content_img = image_loader(CONTENT_PATH).type(dtype).to(device)#.squeeze()\n",
    "    input_img = content_img.clone() #把内容图作为输入图，不是说好的白噪声么\n",
    "#     print(content_img.shape)\n",
    "    model, style_losses, content_losses, output = run_style_transfer(cnn, content_img, style_img, input_img, STEP)\n",
    "    img = output.type(dtype).to(device)\n",
    "    a = image_unloader(img)\n",
    "    a.save(os.path.join(SAVE_PATH, img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
